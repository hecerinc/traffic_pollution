---
title: "Muvit: Traffic patterns and pollution"
output: html_notebook
---

## Introduction

> Describe los datos, es decir, que información contiene y de donde obtuviste la base de datos que vas a utilizar. Incluye una descripción del problema (preguntas que quieren resolver).

<!-- Some intro on why we should be analyzing traffic pattern data -->

### Downloading the data

The pollution data was downloaded using the following bash script: 

```bash
#/bin/bash
count=0
NAM=("santacatarina" "sanbernabe" "obispado" "sannicolas" "pastora" "escobedo" "garcia" "juarez" "sanpedro" "universidad" "puebloserena")
ARR=(31 33 24 28 30 15 22 30 29 19 15)
for a in 139 140 141 142 143 144 145 147 148 425 426; do
		mkdir ${NAM[$count]}
		for i in `seq 1 ${ARR[$count]}`; do
				printf -v j "%02d" $i
				echo "${NAM[$count]}/page_$j.json"
				curl -o "${NAM[$count]}/page_$j.json" "https://api.datos.gob.mx/v2/sinaica?estacionesid=$a&pageSize=1000&page=$i"
		done
		count=$((count+1))
done

```


### Requirements

First we'll load the libraries. We'll be using them for data manipulation, json parsing and graphing, respectively. 

```{r}
library(dplyr)
library(jsonlite)
library(ggplot2)
```


### Reading in the data 


#### Pollution

```{r}
dirs <- list.dirs(recursive = FALSE)
pollution <- do.call(bind_rows, lapply(dirs, function(x){
	files <- dir(x)
	do.call(bind_rows, lapply(files, function(y){
		fromJSON(readLines( paste0(x, "/", y) ))$results
	}))
}))
```



## Exploratory Analysis

### Pollution (SINAICA)

One of the biggest challenges of using the SINAICA pollution data is that their API is not documented anywhere, so we have to look at the data to find out what we've got. 


```{r}
str(pollution)
```

We've got 288K+ rows and 12 columns, which is decently sized. We now coerce the columns into their correct data types, namely the `fecha`, `date` and `date-insert` columns. 

```{r}
pollution$fecha <- as.Date(pollution$fecha)
pollution$parametro <- as.factor(pollution$parametro)
pollution$estacionesid <- as.factor(pollution$estacionesid)
pollution$`date-insert` <- as.POSIXct(pollution$`date-insert`, format="%Y-%m-%dT%H:%M:%OSZ", tz="GMT")
```


We can already see from the data that `_id` (it's a MongoDB-generated identifier), `city`, and `state` will probably not be useful, but we can confirm that, and we need to inspect `date` to see what type of data it holds:

```{r}
unique(pollution$city) # All possible values of city
unique(pollution$state) # All possible values of state
head(pollution$date[!is.na(pollution$date)]) # first values where `date` isn't NA
```


We already know that we're working with Monterrey, Nuevo Leon, so we can omit that. `date` has more datetime information, so we can coerce that as well:

```{r}
pollution <- select(pollution, -city, -state, -`_id`)
pollution$date <- as.POSIXct(pollution$date, format="%Y-%m-%dT%H:%M:%OSZ", tz="GMT")
str(pollution)
```

We have then a total of 12 weather stations that recorded data for a total of 7 pollutants. We also want to know what temporal range the data has:

```{r}
range(pollution$fecha)
range(pollution$date, na.rm = TRUE)
range(pollution$`date-insert`)
levels(pollution$parametro) # pollutants
levels(pollution$estacionesid) # stations
summary(pollution$validoorig)
```

For the whole range of data available in the API, there's roughly a range of 10 months available (January 2017 to April 2018). 

We can also get rid of `validoorig` as it provides no useful information.


```{r}
pollution <- select(pollution, -validoorig)
```

The final dataset we'll be working with looks like this:

```{r}
head(pollution)
```


The stations were extracted from the interactive front-end tool that CENACE makes available at [http://sinaica.inecc.gob.mx/](http://sinaica.inecc.gob.mx/): 


| Station ID | Station name |
| ---------- | ------------ |
| 146 | Apodaca |
| 144 | Escobedo |
| 145 | García |
| 147 | Juárez |
| 143 | La Pastora |
| 141 | Obispado |
| 426 | Pueblo Serena |
| 140 | San Bernabé |
| 142 | San Nicolás |
| 139 | Santa Catarina |
| 148 | San Pedro |
| 425 | Universidad |


The pollutants are then explained by the following table:


| Pollutant code | Pollutant name | 
| -------------- | -------------- |
| `CO` | Carbon monoxide |
| `NO2` | Nitrogen dioxide |
| `O3` | Ozone |
| `PM10` | _PM10 describes inhalable particles, with diameters that are generally 10 micrometers and smaller._ (epa.gov) |
| `PM2.5` | _PM2.5 describes fine inhalable particles, with diameters that are generally 2.5 micrometers and smaller._ (epa.gov) |
| `SO2` | Sulfur dioxide |
| `TMP` | Temperature |


#### Column reference

| Column | Description | 
| ------ | ----------- |
| `id` | An id generated by CENACE. Format: `[STATIONID][POLLUTANT_CODE][YYMMDD][HOUR]` |
| `date-insert` | _Missing_ |
| `parametro` | Measured pollutant code. See table 2.1 |
| `valororig` | Measurement value for pollutant | 
| `estacionesid` | Station ID |
| `hora` | Hour for which the pollutant was measured |
| `fecha` | Date for which the pollutant was measured |
| `date` | _Missing_ |


Those statisticla


### Traffic Jams (Waze)

Objetivo
Qué preguntas tienes, que quieres saber.



## Statistical Analysis

To run a thorough statistical analysis, we will need to load a few more libraries:

```{r}
options(scipen=999) # Disable scientific notation
library(nortest)
library(moments)
```


### Descriptive statistics

#### Pollution

```{r}
pollutants <- levels(pollution$parametro)
```


```{r}
calculateStats <- function(pollutant) {
	poll <- filter(pollution, parametro == pollutant)
	data.frame(pollutant = pollutant, mean = mean(poll$valororig, na.rm=T), variance = var(poll$valororig, na.rm=T), standard.deviation = sd(poll$valororig, na.rm=T), skewness = skewness(poll$valororig, na.rm=T), kurtosis = kurtosis(poll$valororig, na.rm=T))
}
```


```{r}
do.call(rbind, lapply(pollutants, calculateStats))
```

# INTERPRETATION HERE

> De las variables que consideraste interesantes en la etapa 1. Incluye en esta sección la interpretación de los estadísticos descriptivos sobre dichas variables. Para cada una de las variables, incluye también tu inferencia sobre que distribución de probabilidad explica el comportamiento de la variable.

##### Histograms

```{r}
theme_update(plot.title = element_text(hjust = 0.5)) # set titles centered
```



**1. PM 2.5**

```{r fig8, fig.align="center"}
pm25.hist <- ggplot(pollution[pollution$parametro == "PM2.5", ], aes(valororig)) + geom_histogram(binwidth=2, fill="steelblue") + coord_cartesian(xlim = c(0, 100)) + labs(title = "PM2.5", y = "Frequency", x = "Measurement")
pm25.hist
```

**2. CO**

```{r}
ggplot(pollution[pollution$parametro == "CO" & pollution$valororig >= 0, ], aes(valororig)) + geom_histogram(binwidth=.5, fill="steelblue") + coord_cartesian(xlim = c(0, 10)) + labs(title = "Carbon Monoxide (CO)", y = "Frequency", x = "Measurement")
```

**3.NO2**

```{r}
ggplot(filter(pollution, parametro == "NO2"), aes(valororig)) + geom_histogram(fill="steelblue") + labs(title = "Nitrogen Dioxide (NO2)", y = "Frequency", x = "Measurement")
```


**4. O3**

```{r}
ggplot(filter(pollution, parametro == "O3"), aes(valororig)) + geom_histogram(bins=30, fill="steelblue") + labs(title = "Ozone (O3)", y = "Frequency", x = "Measurement")
```

**5. PM10**

```{r}
ggplot(filter(pollution, parametro == "PM10"), aes(valororig)) + geom_histogram(fill="steelblue", binwidth=5) + labs(title = "PM10", y = "Frequency", x = "Measurement") 
```

It's very interesting to note here that the tail runs really long because of some very high values. It's very unusual having such high values for PM10, but since we have a lot of them we can't discard them just yet.

```{r}
head(arrange(filter(pollution, parametro == "PM10"), -valororig), 15)
```



**6. SO2**

```{r}
ggplot(filter(pollution, parametro == "SO2"), aes(valororig)) + geom_histogram(fill="steelblue", bins = 60) + labs(title = "Sulfur Dioxide (SO2)", y = "Frequency", x = "Measurement") + coord_cartesian(xlim = c(0, .05))
```


**7. TMP**
```{r}
ggplot(filter(pollution, parametro == "TMP"), aes(valororig)) + geom_histogram(fill="steelblue", bins=30) + labs(title = "Temperature (TMP)", y = "Frequency", x = "Measurement")
```


#### Waze



### Data normality

We can now do a more formal assesment of the normality of each variable.

```{r fig11, fig.width=10}
ggplot(pollution, aes(sample=valororig)) + stat_qq(size=.5) + facet_wrap('parametro', ncol=3, scales="free")
```

From visual inspection we can see that very few of our variables behave normally.

Running the Shapiro-Wilk test for random samples: 

```{r}
options(scipen=10)
normalvalues <- do.call(rbind, lapply(pollutants, function(poll) data.frame(t(replicate(10, shapiro.test(sample_n(filter(pollution, parametro == poll), 2000)$valororig)$p.value)))))
cbind(pollutants, normalvalues)
```

The Shapiro-Wilks test shows that for every single run of randomly sampled values (2K because it has an uppper limit of 5K and we have a lot more data points), the null hypothesis is rejected (i.e. the p-value is < 0.5), hence none of the random variables in the pollution dataset behave normally.





### Distribution adjustment

```{r}
library(fitdistrplus)
library(actuar) # pareto and weibull
```


We'll now attempt to adjust the data to each of the following distributions:

i)	Exponencial
ii)	Gamma
iii) Weibull
iv)	Lognormal
v)	Pareto



```{r}
exponentials <- lapply(pollutants, function(poll) fitdist(filter(pollution, parametro == poll & valororig > 0)$valororig, "exp"))

gammas <- lapply(pollutants, function(poll) fitdist(filter(pollution, parametro == poll & valororig > 0)$valororig, "gamma", lower=c(0,0), method="mle"))

weibulls <- lapply(pollutants, function(poll) fitdist(filter(pollution, parametro == poll & valororig > 0)$valororig, "weibull", lower=c(0,0)))

lnorms <- lapply(pollutants, function(poll) fitdist(filter(pollution, parametro == poll & valororig > 0)$valororig, "lnorm"))

paretos <- lapply(pollutants, function(poll) fitdist(filter(pollution, parametro == poll & valororig > 0)$valororig, "pareto", start=list(shape=10, scale=500), lower=1, upper=Inf))
```

#### CO


```{r fig1, fig.width=10, fig.height = 8}
par(mfrow = c(2, 2))
plot.legend <- c("Exponential", "Gamma", "Weibull", "lognormal", "Pareto")
fits.cos <- list(exponentials[[1]], gammas[[1]], weibulls[[1]], lnorms[[1]], paretos[[1]])
denscomp(fits.cos, legendtext = plot.legend)
qqcomp(fits.cos, legendtext = plot.legend)
cdfcomp(fits.cos, legendtext = plot.legend)
ppcomp(fits.cos, legendtext = plot.legend)
```

```{r}
gofstat(fits.cos, fitnames=plot.legend)
```


For Carbon Monoxide, the plots suggest it is closer to a Weibull distribution, as evidenced by the CDF plot. This result is further supported by the KS statistic, which has the lowest value at the Weibull. However, this test is not statistically significant for an alpha of 0.5, which is evident from the Q-Q plot (quantile-quantile), and the Anderson-Darling statistics, which show very elevated values and emphasize the lack-of-fit at the talis of the distributions. 

For the evaluated distributions then, we cannot confidently say that Carbon Monoxide readings approach any of these distributions' behaviours.

We then need must try to fit the data to another distribution that might better explain the data.

# FUTHER FITTING



#### NO2

```{r fig2, fig.width=10, fig.height=8}
par(mfrow = c(2, 2))
plot.legend <- c("Exponential", "Gamma", "Weibull", "lognormal", "Pareto")
fits.nos <- list(exponentials[[2]], gammas[[2]], weibulls[[2]], lnorms[[2]], paretos[[2]])
denscomp(fits.nos, legendtext = plot.legend)
qqcomp(fits.nos, legendtext = plot.legend)
cdfcomp(fits.nos, legendtext = plot.legend)
ppcomp(fits.nos, legendtext = plot.legend)
```

```{r}
gofstat(fits.nos, fitnames=plot.legend)
```

For **Nitrogen Dioxide**, the KS statistic gives the lowest value for the lognormal distribution. As with carbon monoxide, however, this value is still not significant enough to accept the null hypothesis, and although the AD statistic is the lowest for this particular distribution, it is a poor fit for both tails and distribution center.

# FURTHER exploration

#### O3

```{r fig3, fig.width=10, fig.height=8}
par(mfrow = c(2, 2))
fits.o3 <- list(exponentials[[3]], gammas[[3]], weibulls[[3]], lnorms[[3]], paretos[[3]])
denscomp(fits.o3, legendtext = plot.legend)
qqcomp(fits.o3, legendtext = plot.legend)
cdfcomp(fits.o3, legendtext = plot.legend)
ppcomp(fits.o3, legendtext = plot.legend)
```

```{r}
gofstat(fits.o3, fitnames = plot.legend)
```

For **Ozone** (O3), it is immediately evident from all four plots that the readings approach a Weibull or Gamma distribution. Choosing between the two is not as evident a task, as both the KS test and the Cramer-von Mises test suggest the Weibull has a better fit, while the Anderson-Darling, AIC, and BIC values suggest the Gamma is a better fit (although the latter two by a very small margin).

In this case, the Q-Q plot suggests a better fit at the tails for the Weibull, while the P-P plot suggests very similar fits for the distribution centers, so we shall choose the **Weibull**.


#### PM10

```{r fig4, fig.width=10, fig.height=8}
par(mfrow = c(2, 2))
fits.pm10 <- list(exponentials[[4]], gammas[[4]], weibulls[[4]], lnorms[[4]], paretos[[4]])
denscomp(fits.pm10, legendtext = plot.legend)
qqcomp(fits.pm10, legendtext = plot.legend)
cdfcomp(fits.pm10, legendtext = plot.legend)
ppcomp(fits.pm10, legendtext = plot.legend)
```

```{r}
gofstat(fits.pm10, fitnames=plot.legend)
```


For **PM10** values, the readings are very clearly best approached by the **lognormal**, as is suggested by the KS statistic, where the lognormal is the only statistically significant value. The Cramer-von Mises (CVM) and Anderson-Darling (AD) statistics, as well as the AIC and BIC, all further support this statement.

An interesting thing to note is that the Q-Q plot suggests high lack-of-fit for all 5 fitted distributions at the tails, which is probably explained by the high right-skewed behaviour of the data, which is highly concentrated on smaller values.


#### PM2.5

```{r fig5, fig.width=10, fig.height=8}
par(mfrow = c(2, 2))
fits.pm25 <- list(exponentials[[5]], gammas[[5]], weibulls[[5]], lnorms[[5]], paretos[[5]])
denscomp(fits.pm25, legendtext = plot.legend)
qqcomp(fits.pm25, legendtext = plot.legend)
cdfcomp(fits.pm25, legendtext = plot.legend)
ppcomp(fits.pm25, legendtext = plot.legend)
```

```{r}
gofstat(fits.pm25, fitnames=plot.legend)
```

PM 2.5 values are also very highly concentrated on smaller values, with a few exceptions of really high readings, which can be better appreciated in the previous histogram, which focused on the smaller values:

```{r fig12, fig.width = 4}
pm25.hist
```

Whilst the KS and CVM sstatistics suggest a Weibull approximation, they're not statistically significant, evidenced by the AD value which (at Inf), suggest a terrible approximation at the distribution tails. It's interesting to note that while the KS and CVM suggest a Weibull distribution, followed perhaps by the Gamma, both the AIC and BIC criteria suggest a lognormal to be a better fit. 


# ADJUSTING FOR SOME OTHER DISTRIBUTIONS,


#### SO2

```{r fig6, fig.width=10, fig.height=8}
par(mfrow = c(2, 2))
fits.so2 <- list(exponentials[[6]], gammas[[6]], weibulls[[6]], lnorms[[6]], paretos[[6]])
denscomp(fits.so2, legendtext = plot.legend)
qqcomp(fits.so2, legendtext = plot.legend)
cdfcomp(fits.so2, legendtext = plot.legend)
ppcomp(fits.so2, legendtext = plot.legend)
```


```{r}
gofstat(fits.so2, fitnames = plot.legend)
```


For **Sulfur Dioxide**, the KS statistic best approaches a **lognormal** distribution, a theory that's supported by the CVM and AD statistics, as well as the AIC and BIC. Note that the KS statistic is not entirely within the non-rejection threshold, which is evidenced by the Q-Q plot which suggests lack-of-fit at the distribution tails and which can be in turn plainly seen in both the CDF and density plots.




#### TMP

```{r fig7, fig.width=10, fig.height=8}
par(mfrow = c(2, 2))
fits.tmp <- list(exponentials[[7]], gammas[[7]], weibulls[[7]], lnorms[[7]], paretos[[7]])
denscomp(fits.tmp, legendtext = plot.legend)
qqcomp(fits.tmp, legendtext = plot.legend)
cdfcomp(fits.tmp, legendtext = plot.legend)
ppcomp(fits.tmp, legendtext = plot.legend)
```

```{r}
gofstat(fits.tmp, fitnames = plot.legend)
```

For **temperature** (TMP), the statistics, criteria and graphs all immediately suggest a **Weibull** distribution.


### Critical variable


## Bayesian Network Analysis



## Discussion

## Conclusions

















