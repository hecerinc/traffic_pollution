---
title: "Muvit: Traffic patterns and pollution"
output: html_notebook
---

## Introduction

> Describe los datos, es decir, que información contiene y de donde obtuviste la base de datos que vas a utilizar. Incluye una descripción del problema (preguntas que quieren resolver).

<!-- Some intro on why we should be analyzing traffic pattern data -->

### Downloading the data

The pollution data was downloaded using the following bash script: 

```bash
#/bin/bash
count=0
NAM=("santacatarina" "sanbernabe" "obispado" "sannicolas" "pastora" "escobedo" "garcia" "juarez" "sanpedro" "universidad" "puebloserena")
ARR=(31 33 24 28 30 15 22 30 29 19 15)
for a in 139 140 141 142 143 144 145 147 148 425 426; do
		mkdir ${NAM[$count]}
		for i in `seq 1 ${ARR[$count]}`; do
				printf -v j "%02d" $i
				echo "${NAM[$count]}/page_$j.json"
				curl -o "${NAM[$count]}/page_$j.json" "https://api.datos.gob.mx/v2/sinaica?estacionesid=$a&pageSize=1000&page=$i"
		done
		count=$((count+1))
done

```


### Requirements

First we'll load the libraries. We'll be using them for data manipulation, json parsing and graphing, respectively. 

```{r}
library(dplyr)
library(jsonlite)
library(ggplot2)
```


### Reading in the data 


#### Pollution

```{r}
dirs <- list.dirs(recursive = FALSE)
pollution <- do.call(bind_rows, lapply(dirs, function(x){
	files <- dir(x)
	do.call(bind_rows, lapply(files, function(y){
		fromJSON(readLines( paste0(x, "/", y) ))$results
	}))
}))
```



## Exploratory Analysis

### Pollution (SINAICA)

One of the biggest challenges of using the SINAICA pollution data is that their API is not documented anywhere, so we have to look at the data to find out what we've got. 


```{r}
str(pollution)
```

We've got 288K+ rows and 12 columns, which is decently sized. We now coerce the columns into their correct data types, namely the `fecha`, `date` and `date-insert` columns. 

```{r}
pollution$fecha <- as.Date(pollution$fecha)
pollution$parametro <- as.factor(pollution$parametro)
pollution$estacionesid <- as.factor(pollution$estacionesid)
pollution$`date-insert` <- as.POSIXct(pollution$`date-insert`, format="%Y-%m-%dT%H:%M:%OSZ", tz="GMT")
```


We can already see from the data that `_id` (it's a MongoDB-generated identifier), `city`, and `state` will probably not be useful, but we can confirm that, and we need to inspect `date` to see what type of data it holds:

```{r}
unique(pollution$city) # All possible values of city
unique(pollution$state) # All possible values of state
head(pollution$date[!is.na(pollution$date)]) # first values where `date` isn't NA
```


We already know that we're working with Monterrey, Nuevo Leon, so we can omit that. `date` has more datetime information, so we can coerce that as well:

```{r}
pollution <- select(pollution, -city, -state, -`_id`)
pollution$date <- as.POSIXct(pollution$date, format="%Y-%m-%dT%H:%M:%OSZ", tz="GMT")
str(pollution)
```

We have then a total of 12 weather stations that recorded data for a total of 7 pollutants. We also want to know what temporal range the data has:

```{r}
range(pollution$fecha)
range(pollution$date, na.rm = TRUE)
range(pollution$`date-insert`)
levels(pollution$parametro) # pollutants
levels(pollution$estacionesid) # stations
summary(pollution$validoorig)
```

For the whole range of data available in the API, there's roughly a range of 10 months available (January 2017 to April 2018). 

We can also get rid of `validoorig` as it provides no useful information.


```{r}
pollution <- select(pollution, -validoorig)
```

The final dataset we'll be working with looks like this:

```{r}
head(pollution)
```


The stations were extracted from the interactive front-end tool that CENACE makes available at [http://sinaica.inecc.gob.mx/](http://sinaica.inecc.gob.mx/): 


| Station ID | Station name |
| ---------- | ------------ |
| 146 | Apodaca |
| 144 | Escobedo |
| 145 | García |
| 147 | Juárez |
| 143 | La Pastora |
| 141 | Obispado |
| 426 | Pueblo Serena |
| 140 | San Bernabé |
| 142 | San Nicolás |
| 139 | Santa Catarina |
| 148 | San Pedro |
| 425 | Universidad |


The pollutants are then explained by the following table:


| Pollutant code | Pollutant name | 
| -------------- | -------------- |
| `CO` | Carbon monoxide |
| `NO2` | Nitrogen dioxide |
| `O3` | Ozone |
| `PM10` | _PM10 describes inhalable particles, with diameters that are generally 10 micrometers and smaller._ (epa.gov) |
| `PM2.5` | _PM2.5 describes fine inhalable particles, with diameters that are generally 2.5 micrometers and smaller._ (epa.gov) |
| `SO2` | Sulfur dioxide |
| `TMP` | Temperature |


#### Column reference

| Column | Description | 
| ------ | ----------- |
| `id` | An id generated by CENACE. Format: `[STATIONID][POLLUTANT_CODE][YYMMDD][HOUR]` |
| `date-insert` | _Missing_ |
| `parametro` | Measured pollutant code. See table 2.1 |
| `valororig` | Measurement value for pollutant | 
| `estacionesid` | Station ID |
| `hora` | Hour for which the pollutant was measured |
| `fecha` | Date for which the pollutant was measured |
| `date` | _Missing_ |


Those statisticla


### Traffic Jams (Waze)

Objetivo
Qué preguntas tienes, que quieres saber.



## Statistical Analysis

To run a thorough statistical analysis, we will need to load a few more libraries:

```{r}
library(nortest)
library(moments)
```


### Descriptive statistics

#### Pollution

```{r}
pollutants <- levels(pollution$parametro)
```


```{r}
calculateStats <- function(pollutant) {
	poll <- filter(pollution, parametro == pollutant)
	data.frame(pollutant = pollutant, mean = mean(poll$valororig, na.rm=T), variance = var(poll$valororig, na.rm=T), standard.deviation = sd(poll$valororig, na.rm=T), skewness = skewness(poll$valororig, na.rm=T), kurtosis = kurtosis(poll$valororig, na.rm=T))
}
```


```{r}
do.call(rbind, lapply(pollutants, calculateStats))
```

##### Histograms

```{r}
theme_update(plot.title = element_text(hjust = 0.5)) # set titles centered
```


```{r}
hists <- ggplot(pollution, aes(valororig))
hists + geom_histogram(binwidth=.5)
```

**1. PM 2.5**

```{r}
ggplot(pollution[pollution$parametro == "PM2.5", ], aes(valororig)) + geom_histogram(binwidth=2, fill="steelblue") + coord_cartesian(xlim = c(0, 100)) + labs(title = "PM2.5", y = "Frequency", x = "Measurement")
```

**2. CO**

```{r}
ggplot(pollution[pollution$parametro == "CO" & pollution$valororig >= 0, ], aes(valororig)) + geom_histogram(binwidth=.5, fill="steelblue") + coord_cartesian(xlim = c(0, 10)) + labs(title = "Carbon Monoxide (CO)", y = "Frequency", x = "Measurement")
```

**3.NO2**

```{r}
ggplot(filter(pollution, parametro == "NO2"), aes(valororig)) + geom_histogram(fill="steelblue") + labs(title = "Nitrogen Dioxide (NO2)", y = "Frequency", x = "Measurement")
```


**4. O3**

```{r}
ggplot(filter(pollution, parametro == "O3"), aes(valororig)) + geom_histogram(bins=30, fill="steelblue") + labs(title = "Ozone (O3)", y = "Frequency", x = "Measurement")
```

**5. PM10**

```{r}
ggplot(filter(pollution, parametro == "PM10"), aes(valororig)) + geom_histogram(fill="steelblue", binwidth=5) + labs(title = "PM10", y = "Frequency", x = "Measurement") 
```

It's very interesting to note here that the tail runs really long because of some very high values. It's very unusual having such high values for PM10, but since we have a lot of them we can't discard them just yet.

```{r}
head(arrange(filter(pollution, parametro == "PM10"), -valororig), 15)
```



**6. SO2**

```{r}
ggplot(filter(pollution, parametro == "SO2"), aes(valororig)) + geom_histogram(fill="steelblue", bins = 60) + labs(title = "Sulfur Dioxide (SO2)", y = "Frequency", x = "Measurement") + coord_cartesian(xlim = c(0, .05))
```


**7. TMP**
```{r}
ggplot(filter(pollution, parametro == "TMP"), aes(valororig)) + geom_histogram(fill="steelblue", bins=30) + labs(title = "Temperature (TMP)", y = "Frequency", x = "Measurement")
```


#### Waze



### Data normality

### Distribution adjustment

### Critical variable


## Bayesian Network Analysis



## Discussion

## Conclusions

















